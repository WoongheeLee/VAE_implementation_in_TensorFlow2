{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 목적\n",
    "VAE 구현\n",
    "\n",
    "[코드 참고](https://datascienceschool.net/view-notebook/c5248de280a64ae2a96c1d4e690fdf79/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from env import *\n",
    "\n",
    "TRAIN = True\n",
    "ETA = 1e-5\n",
    "DEVICE = \"0\"\n",
    "SEED = 0\n",
    "\n",
    "project_path = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = DEVICE\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import models, layers, losses, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이타 로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255.\n",
    "X_train = X_train.reshape(X_train.shape + (1,))\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "X_test = X_test.reshape(X_test.shape + (1,))\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 2\n",
    "ETA = 1e-4\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 2**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(models.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.padding = 'same'\n",
    "        \n",
    "        self.c1 = layers.Conv2D(\n",
    "                filters=32,\n",
    "                kernel_size=3,\n",
    "                padding=self.padding,\n",
    "                activation='relu')\n",
    "        self.c2 = layers.Conv2D(\n",
    "                filters=64,\n",
    "                kernel_size=3,\n",
    "                padding=self.padding,\n",
    "                activation='relu')\n",
    "        self.c3 = layers.Conv2D(\n",
    "                filters=64,\n",
    "                kernel_size=3,\n",
    "                padding=self.padding,\n",
    "                activation='relu')\n",
    "        self.c4 = layers.Conv2D(\n",
    "                filters=64,\n",
    "                kernel_size=3,\n",
    "                padding=self.padding,\n",
    "                activation='relu')\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(32, activation='relu')\n",
    "        self.dense_z_mean = layers.Dense(latent_dim)\n",
    "        self.z_log_var = layers.Dense(latent_dim)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.c1(inputs)\n",
    "        x = self.c2(x)\n",
    "        x = self.c3(x)\n",
    "        before_flattening = self.c4(x)\n",
    "        x = self.flatten(before_flattening)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        z_mean = self.dense_z_mean(x)\n",
    "        z_log_var = self.z_log_var(x)\n",
    "        \n",
    "        epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], self.latent_dim), mean=.0, stddev=1)\n",
    "        z = z_mean + tf.math.exp(z_log_var) * epsilon\n",
    "        \n",
    "        return z, before_flattening, z_mean, z_log_var\n",
    "    \n",
    "encoder = Encoder(LATENT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(models.Model):\n",
    "    def __init__(self,latent_dim,before_flattening):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.padding = 'same'\n",
    "        self.shape_before_flattening = tf.shape(before_flattening)\n",
    "        \n",
    "        self.dense = layers.Dense(np.prod(self.shape_before_flattening[1:]),\n",
    "                                 activation='relu')\n",
    "        self.reshape = layers.Reshape(self.shape_before_flattening[1:])\n",
    "        self.conv2dtranspose = layers.Conv2DTranspose(\n",
    "            filters=32,\n",
    "            kernel_size=3,\n",
    "            padding=self.padding,\n",
    "            activation='relu',\n",
    "            strides=(1,1)\n",
    "        )\n",
    "        self.conv2d = layers.Conv2D(\n",
    "            filters=1,\n",
    "            kernel_size=3,\n",
    "            padding=self.padding,\n",
    "            activation='sigmoid'\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.dense(inputs)\n",
    "        x = self.reshape(x)\n",
    "        x = self.conv2dtranspose(x)\n",
    "        z_decoded = self.conv2d(x)\n",
    "        return z_decoded\n",
    "\n",
    "temp_before_flattening = np.ones((1,28,28,64))\n",
    "decoder = Decoder(LATENT_DIM,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = losses.BinaryCrossentropy()\n",
    "opt = optimizers.Adam(learning_rate=ETA)\n",
    "\n",
    "def vae_loss(x, z_decoded, z_mean, z_log_var):\n",
    "        flatten = layers.Flatten()\n",
    "        x = flatten(x)\n",
    "        z_decoded = flatten(z_decoded)\n",
    "\n",
    "        cross_entropy_loss = bce(x, z_decoded)\n",
    "        kl_loss = -5e-4 \\\n",
    "            * tf.reduce_mean(\n",
    "                1 + z_log_var\\\n",
    "                - tf.math.square(z_mean)\\\n",
    "                - tf.math.exp(z_log_var),\n",
    "                axis=-1)\n",
    "\n",
    "        return cross_entropy_loss + kl_loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in tqdm(range(EPOCHS)):\n",
    "    r = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(r)\n",
    "    X_train = X_train[r]\n",
    "    \n",
    "    loss_sum = .0\n",
    "    for start in range(0,X_train.shape[0],BATCH_SIZE):\n",
    "        end = min(start+BATCH_SIZE, X_train.shape[0])\n",
    "        X_batch = X_train[start:end]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            z, _, z_mean, z_log_var = encoder(X_batch)\n",
    "            z_decoded = decoder(z)\n",
    "            loss = vae_loss(X_batch, z_decoded, z_mean, z_log_var)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            loss, \n",
    "            [encoder.trainable_variables,decoder.trainable_variables])\n",
    "        opt.apply_gradients(zip(grads[0], encoder.trainable_variables))\n",
    "        opt.apply_gradients(zip(grads[1], decoder.trainable_variables))\n",
    "        \n",
    "        loss_sum += np.sum(loss)\n",
    "        \n",
    "    if (e+1) % 10 == 0:\n",
    "        print(f'{e+1}/{EPOCHS}, loss: {loss_sum:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, _, z_mean, z_log_var = encoder(X_test[:1])\n",
    "z_decoded = decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = X_test[:1]\n",
    "print(temp.shape, z_decoded.shape)\n",
    "temp = np.squeeze(temp)\n",
    "z_decoded = np.squeeze(z_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(temp, cmap='Greys_r')\n",
    "plt.title('Original')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(z_decoded, cmap='Greys_r')\n",
    "plt.title('Decoded')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
